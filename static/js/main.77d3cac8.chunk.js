(this.webpackJsonpbase=this.webpackJsonpbase||[]).push([[4],{13:function(e,a,t){"use strict";t.d(a,"e",(function(){return i})),t.d(a,"c",(function(){return n})),t.d(a,"d",(function(){return o})),t.d(a,"b",(function(){return r})),t.d(a,"a",(function(){return d})),t.d(a,"f",(function(){return s}));var i,n,o,r,s,d,l=Object({NODE_ENV:"production",PUBLIC_URL:"",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_TEAM_ACHIEVEMENTS:'[{"_id":"614dbac1c361a52ac01632f8","title":"bfgdbdfgbfgb","yearAwarded":1234,"description":"bsdgfbsgfdb","teamId":"6149d66a15738504e4babae3","__v":0},{"_id":"614c226191df5b097896f8e5","title":"fsadfasdf","yearAwarded":1234,"description":"asdfasdfasd","teamId":"6149d66a15738504e4babae3","__v":0},{"_id":"614dbacac361a52ac01632fb","title":"rwerwerewr","yearAwarded":1232,"description":"234234234","teamId":"6149d66a15738504e4babae3","__v":0}]',REACT_APP_TEAM_HOMEPAGE:'{"aboutUs":[""],"_id":"6149d66b15738504e4babae7","teamId":"6149d66a15738504e4babae3","createdAt":"2021-09-21T12:56:11.101Z","updatedAt":"2021-09-21T12:56:40.335Z","__v":0}',REACT_APP_TEAM_INFO:'{"_id":"6149d66a15738504e4babae3","teamName":"ppp","orgName":"ooo","email":"pmyip3@gmail.com","profilePic":"data:image/jpeg;base64,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"}',REACT_APP_TEAM_MEMBERS:'[{"_id":"614c225491df5b097896f8da","fullName":"fasdf","position":"asdfas","summary":"dfasdf"},{"_id":"614dbad1c361a52ac01632ff","fullName":"greg","position":"rtgreg","summary":"rtegretgretg"},{"_id":"614dbad5c361a52ac0163307","fullName":"gretgre","position":"tgretg","summary":"retgrtegrteg"}]',REACT_APP_TEAM_PUBLICATIONS:'[{"_id":"614c70e93ab34c6af0eef502","authors":["Xiao Chen","Wanli Chen","Kui Liu","Chunyang Chen","Li Li"],"title":"A comparative study of smartphone and smartwatch apps","link":"https://kui-liu.github.io/papers/chen2021comparative.pdf","description":"Despite that our community has spent numerous efforts on analyzing mobile apps, there is no study proposed for characterizing the relationship between smartphone and smartwatch apps. To fill this gap, we present to the community a comparative study of smartphone and smartwatch apps, aiming at understanding the status quo of cross-phone/watch apps. Specifically, in this work, we first collect a set of cross-phone/watch app pairs and then experimentally look into them to explore their similarities or dissimilarities from different perspectives. Experimental results show that (1) Approximately, up to 40% of resource files, 30% of code methods are reused between smartphone/watch app pairs,(2) Smartphone apps may require more than twice as many as permissions and adopt more than five times as many as user interactions than their watch counterparts, and (3) Smartwatch apps can be released as either\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Book","categoryTitle":"Proceedings of the 36th Annual ACM Symposium on Applied Computing","pages":"1484-1493","publisher":"","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.441Z","updatedAt":"2021-09-23T12:19:53.441Z","year":2021},{"_id":"614c70e93ab34c6af0eef501","authors":["Sidong Feng","Suyu Ma","Jinzhong Yu","Chunyang Chen","TingTing Zhou","Yankun Zhen"],"title":"Auto-icon: An automated code generation tool for icon designs assisting in ui development","link":"https://chunyang-chen.github.io/publication/icon_IUI21.pdf","description":"Approximately 50% of development resources are devoted to UI development tasks [8]. Occupied a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this study, we define 100 icon classes through an iterative open coding for the existing icon design sharing website. Based on a deep learning model and computer vision methods, we propose an approach to automatically convert icon images to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon images, in terms of saving 65.2\xa0\u2026","yearPublished":"2021","citedBy":120211,"category":{"type":"Book","categoryTitle":"26th International Conference on Intelligent User Interfaces","pages":"59-69","publisher":"","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.441Z","updatedAt":"2021-09-23T12:19:53.441Z","year":2021},{"_id":"614c70e93ab34c6af0eef4ff","authors":["Junjie Wang","Ye Yang","Song Wang","Chunyang Chen","Dandan Wang","Qing Wang"],"title":"Context-aware Personalized Crowdtesting Task Recommendation","link":"","description":"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker\'s failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkers\' perspectives. We motivate this study through a pilot study, revealing the large portion (74\\\\%) of unpaid crowdworkers\'\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Journal","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.441Z","updatedAt":"2021-09-23T12:19:53.441Z","year":2021},{"_id":"614c70e93ab34c6af0eef503","authors":["Han Wang","Chunyang Chen","Zhenchang Xing","John Grundy"],"title":"DiffTech: Differencing Similar Technologies from Crowd-Scale Comparison Discussions","link":"https://chunyang-chen.github.io/publication/diffTech_TSE21.pdf","description":"Developers use different technologies for many software development tasks. However, when faced with several technologies with comparable functionalities, it is not easy to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites. However, it still remains difficult to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the DiffTech system that exploits crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different aspects. We first build a large database of comparable technologies in software engineering by mining tags in Stack Overflow. We then locate comparative sentences about comparable technologies with\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Journal","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.441Z","updatedAt":"2021-09-23T12:19:53.441Z","year":2021},{"_id":"614c70e93ab34c6af0eef500","authors":["Qiuyuan Chen","Chunyang Chen","Safwat Hassan","Zhengchang Xing","Xin Xia","Ahmed E Hassan"],"title":"How Should I Improve the UI of My App? A Study of User Reviews of Popular Apps in the Google Play","link":"","description":"UI (User Interface) is an essential factor influencing users\u2019 perception of an app. However, it is hard for even professional designers to determine if the UI is good or not for end-users. Users\u2019 feedback (e.g., user reviews in the Google Play) provides a way for app owners to understand how the users perceive the UI. In this article, we conduct an in-depth empirical study to analyze the UI issues of mobile apps. In particular, we analyze more than 3M UI-related reviews from 22,199 top free-to-download apps and 9,380 top non-free apps in the Google Play Store. By comparing the rating of UI-related reviews and other reviews of an app, we observe that UI-related reviews have lower ratings than other reviews. By manually analyzing a random sample of 1,447 UI-related reviews with a 95% confidence level and a 5% interval, we identify 17 UI-related issues types that belong to four categories (i.e., \u201cAppearance\xa0\u2026","yearPublished":"2021","citedBy":320213,"category":{"type":"Journal","categoryTitle":"ACM Transactions on Software Engineering and Methodology (TOSEM)","pages":"1-38","publisher":"ACM","volume":"30","issue":"3"},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.441Z","updatedAt":"2021-09-23T12:19:53.441Z","year":2021},{"_id":"614c70e93ab34c6af0eef4fc","authors":["Yuhui Su","Zhe Liu","Chunyang Chen","Junjie Wang","Qing Wang"],"title":"OwlEyes-online: a fully automated platform for detecting and localizing UI display issues","link":"https://arxiv.org/pdf/2107.02364","description":"Graphical User Interface (GUI) provides visual bridges between software apps and end users. However, due to the compatibility of software or hardware, UI display issues such as text overlap, blurred screen, image missing always occur during GUI rendering on different devices. Because these UI display issues can be found directly by human eyes, in this paper, we implement an online UI display issue detection tool OwlEyes-Online, which provides a simple and easy-to-use platform for users to realize the automatic detection and localization of UI display issues. The OwlEyes-Online can automatically run the app and get its screenshots and XML files, and then detect the existence of issues by analyzing the screenshots. In addition, OwlEyes-Online can also find the detailed area of the issue in the given screenshots to further remind developers. Finally, OwlEyes-Online will automatically generate test reports with UI\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Book","categoryTitle":"Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1500-1504","publisher":"","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.440Z","updatedAt":"2021-09-23T12:19:53.440Z","year":2021},{"_id":"614c70e93ab34c6af0eef508","authors":["Han Wang","Chunyang Chen","Zhenchang Xing","John Grundy"],"title":"DiffTech: A tool for differencing similar technologies from question-and-answer discussions","link":"https://chunyang-chen.github.io/publication/diffTechTool_FSE20.pdf","description":"Developers can use different technologies for different software development tasks in their work. However, when faced with several technologies with comparable functionalities, it can be challenging for developers to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison. However, it is still very opportunistic whether they will get a comprehensive comparison, as online information is often fragmented, contradictory and biased. To overcome these limitations, we propose the DiffTech system that exploits the crowd sourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We found 19,118 comparative sentences from 2,410 pairs of comparable technologies\xa0\u2026","yearPublished":"2020","citedBy":120211,"category":{"type":"Book","categoryTitle":"Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1576-1580","publisher":"","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.442Z","updatedAt":"2021-09-23T12:19:53.442Z","year":2020},{"_id":"614c70e93ab34c6af0eef509","authors":["Mulong Xie","Sidong Feng","Zhenchang Xing","Jieshan Chen","Chunyang Chen"],"title":"UIED: a hybrid tool for GUI element detection","link":"https://www.researchgate.net/profile/Mulong-Xie/publication/346170544_UIED_a_hybrid_tool_for_GUI_element_detection/links/5fbca13a458515b797641b72/UIED-a-hybrid-tool-for-GUI-element-detection.pdf","description":"Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be\xa0\u2026","yearPublished":"2020","citedBy":520215,"category":{"type":"Book","categoryTitle":"Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1655-1659","publisher":"","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.442Z","updatedAt":"2021-09-23T12:19:53.442Z","year":2020},{"_id":"614c70e93ab34c6af0eef50d","authors":["Jieshan Chen","Chunyang Chen","Zhenchang Xing","Xin Xia","Liming Zhu","John Grundy","Jinshui Wang"],"title":"Wireframe-based UI design search through image autoencoder","link":"https://arxiv.org/pdf/2103.07085","description":"UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for\xa0\u2026","yearPublished":"2020","citedBy":1520202021411,"category":{"type":"Journal","categoryTitle":"ACM Transactions on Software Engineering and Methodology (TOSEM)","pages":"1-31","publisher":"ACM","volume":"29","issue":"3"},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.442Z","updatedAt":"2021-09-23T12:19:53.442Z","year":2020},{"_id":"614c70e93ab34c6af0eef511","authors":["Xu Wang","Chunyang Chen","Zhenchang Xing"],"title":"Domain-specific machine translation with recurrent neural network for software localization","link":"https://chunyang-chen.github.io/publication/softwareLocalization_EMSE.pdf","description":" Software localization is the process of adapting a software product to the linguistic, cultural and technical requirements of a target market. It allows software companies to access foreign markets that would be otherwise difficult to penetrate. Many studies have been carried out to locate need-to-translate strings in software and adapt UI layout after text translation in the new language. However, no work has been done on the most important and time-consuming step of software localization process, i.e., the translation of software text. Due to some unique characteristics of software text, for example, application-specific meanings, context-sensitive translation, domain-specific rare words, general machine translation tools such as Google Translate cannot properly address linguistic and technical nuance in translating software text for software localization. In this paper, we propose a neural-network based translation model\xa0\u2026","yearPublished":"2019","citedBy":16201920202021196,"category":{"type":"Journal","categoryTitle":"Empirical Software Engineering","pages":"3514-3545","publisher":"Springer US","volume":"24","issue":"6"},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.443Z","updatedAt":"2021-09-23T12:19:53.443Z","year":2019},{"_id":"614c70e93ab34c6af0eef514","authors":["Suyu Ma","Zhenchang Xing","Chunyang Chen","Cheng Chen","Lizhen Qu","Guoqiang Li"],"title":"Easy-to-deploy API extraction by multi-level feature embedding and transfer learning","link":"https://chunyang-chen.github.io/publication/API_NER_TSE2019.pdf","description":"Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazzetter) beyond the input texts. We also\xa0\u2026","yearPublished":"2019","citedBy":152020202178,"category":{"type":"Journal","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.443Z","updatedAt":"2021-09-23T12:19:53.443Z","year":2019},{"_id":"614c70e93ab34c6af0eef512","authors":["Sen Chen","Lingling Fan","Chunyang Chen","Minhui Xue","Yang Liu","Lihua Xu"],"title":"Gui-squatting attack: Automated generation of android phishing apps","link":"","description":"Mobile phishing attacks, such as mimic mobile browser pages, masquerade as legitimate applications by leveraging repackaging or clone techniques, have caused varied yet significant security concerns. Consequently, detection techniques have been receiving increasing attention. However, many such detection methods are not well tested and may therefore still be vulnerable to new types of phishing attacks. In this paper, we propose a new attacking technique, named GUI-Squatting attack, which can generate phishing apps (phapps) automatically and effectively. Our method adopts image processing and deep learning algorithms, to enable powerful and large-scale attacks. We observe that a successful phishing attack requires two conditions, page confusion and logic deception during attacks synthesis. We directly optimize these two conditions to create a practical attack. Our experimental results reveal that\xa0\u2026","yearPublished":"2019","citedBy":22202020211011,"category":{"type":"Journal","categoryTitle":"IEEE Transactions on Dependable and Secure Computing","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.443Z","updatedAt":"2021-09-23T12:19:53.443Z","year":2019},{"_id":"614c70e93ab34c6af0eef518","authors":["Chunyang Chen","Zhenchang Xing","Yang Liu","Kent Long Xiong Ong"],"title":"Mining likely analogical apis across third-party libraries via large-scale unsupervised api semantics embedding","link":"https://chunyang-chen.github.io/publication/similarAPI_TSE19.pdf","description":"Establishing API mappings between third-party libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined. Having an automatic technique to create a database of likely API mappings can significantly ease the task. Unfortunately, existing techniques either adopt supervised learning mechanism that requires already-ported or functionality similar applications across major programming languages or platforms, which are difficult to come by for an arbitrary pair of third-party libraries, or cannot deal with lexical gap in the API descriptions of different libraries. To overcome these limitations, we present an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. Based on\xa0\u2026","yearPublished":"2019","citedBy":3520192020202161000,"category":{"type":"Journal","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.443Z","updatedAt":"2021-09-23T12:19:53.443Z","year":2019},{"_id":"614c70e93ab34c6af0eef515","authors":["Chunyang Chen","Zhenchang Xing","Yang Liu"],"title":"What\u2019s spain\u2019s paris? mining analogical libraries from q&a discussions","link":"https://chunyang-chen.github.io/publication/analogyJournal_EMSE.pdf","description":"               Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with for different programming languages or different mobile platforms. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q&A posts, which often contain overwhelming or out-of-date information. In this paper, we present a new approach to recommend analogical libraries based on a knowledge base of analogical libraries mined from tags of millions of Stack Overflow questions. The novelty of our approach is to solve analogical-library questions by combining state-of-the-art word embedding technique and domain-specific relational and categorical knowledge mined from Stack Overflow. Given a library and a recommended analogical library, our\xa0\u2026","yearPublished":"2019","citedBy":16201920202021556,"category":{"type":"Journal","categoryTitle":"Empirical Software Engineering","pages":"1155-1194","publisher":"Springer US","volume":"24","issue":"3"},"teamId":"6149d66a15738504e4babae3","__v":0,"createdAt":"2021-09-23T12:19:53.443Z","updatedAt":"2021-09-23T12:19:53.443Z","year":2019}]',REACT_APP_TEAM_SITE_METADATA:'{"template":{"layout":"2","theme":"dark"},"publicationOptions":{"groupBy":"Category","sortBy":"Category Title"},"pages":["PUBLICATIONS","TEAM","ACHIEVEMENTS"],"_id":"6149d66a15738504e4babae5","teamId":"6149d66a15738504e4babae3","createdAt":"2021-09-21T12:56:10.858Z","updatedAt":"2021-09-24T14:46:29.766Z","__v":6}'});l.REACT_APP_DEBUG?(console.log("Running in DEBUG mode, hence using fake data"),i=[{_id:"fake_publication_1",title:"Auto-icon: An automated code generation tool for icon designs assisting in ui development",description:'"Approximately 50% of development resources are devoted to UI development tasks [8]. Occupied a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this study, we define 100 icon classes through an iterative open coding for the existing icon design sharing website. Based on a deep learning model and computer vision methods, we propose an approach to automatically convert icon images to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon images, in terms of saving 65.2 \u2026',category:{type:"BOOK",categoryTitle:"26th International Conference on Intelligent User Interfaces",issue:"",volume:"",pages:"56-69",publisher:""},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Sidong Feng","Suyu Ma","Jinzhong Yu","Chunyang Chen","TingTing Zhou","Yankun Zhen"],yearPublished:"2021"},{_id:"fake_publication_2",title:"Context-aware Personalized Crowdtesting Task Recommendation",description:"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker's failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkers' perspectives. We motivate this study through a pilot study, revealing the large portion (74%) of unpaid crowdworkers' \u2026",category:{type:"JOURNAL",categoryTitle:"International Economics Journal",issue:"10.1",volume:"11.2",pages:"10",publisher:"IMF"},link:"https://www.imf.org/en/Home",authors:["Jeremy Buffet","Warren Graham"],yearPublished:"2021"},{_id:"fake_publication_3",title:"MULTI-PHACET-MULTIdimensional clinical phenotyping of hospitalised acute COPD ExacerbaTions",description:"Securing generative Arts through NFTs Lorem ipsum amet dolor sit amet, minim altera mucius an eum. Etiam feugiat laoreet tempor. Vestibulum vel facilisis odio, in ultricies ex. Nullam vitae lectus vitae arcu efficitur auctor id a sem. Mauris congue enim risus, eu gravida mi dignissim ut. Vestibulum tempus urna vel sem eleifend, quis aliquet ligula maximus. Vivamus sagittis dolor eu iaculis interdum. Morbi ex odio, ornare eget erat eu, dapibus accumsan elit. Fusce fermentum orci ante. Etiam dolor urna, dictum a diam nec, ornare tempor nunc. Curabitur imperdiet malesuada augue eget vestibulum. Legere antiopam definitiones nam an.",category:{type:"BOOK",categoryTitle:"The digital asset certification",issue:"",volume:"",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio"],yearPublished:"2017"},{_id:"fake_publication_4",title:"CONCORDANCE OF LARYNGOSCOPY AND DYNAMIC COMPUTERIZED TOMOGRAPHY LARYNX TO DIAGNOSE VOCAL CORD DYSFUNCTION ",description:"",category:{type:"CONFERENCE",categoryTitle:"RESPIROLOGY",issue:"",volume:"",pages:"102-102",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Kaibo Cao","Chunyang Chen","Sebastian Baltes","Christoph Treude","Xiang Chen"],yearPublished:"2017"},{_id:"fake_publication_5",title:"Rivaroxaban compared to placebo for the treatment of leg superficial vein thrombosis: a randomized trial",description:"The role of rivaroxaban in the treatment of leg superficial venous thrombosis (SVT) is uncertain. This article aims to determine if rivaroxaban is an effective and safe treatment for leg SVT. Patients with symptomatic leg SVT of at least 5\u2009cm length were randomized to 45 days of rivaroxaban 10\u2009mg daily or to placebo, and followed for a total of 90 days. Treatment failure (required a nonstudy anticoagulant; had proximal deep vein thrombosis or pulmonary embolism; or had surgery for SVT) at 90 days was the primary efficacy outcome. Secondary efficacy outcomes included leg pain severity, and venous disease-specific and general health-related quality of life over 90 days. Major bleeding at 90 days was the primary safety outcome. Poor enrollment led to the trial being stopped after 85 of the planned 600 patients were randomized to rivaroxaban (n\u2009=\u200943) or placebo (n\u2009=\u200942). One rivaroxaban and five placebo \u2026",category:{type:"OTHER",categoryTitle:"",issue:"",volume:"54",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio"],yearPublished:"2017"},{_id:"fake_publication_6",title:"Owl Eyes: Spotting UI Display Issues via Visual Understanding",description:"Etiam feugiat laoreet tempor. Vestibulum vel facilisis odio, in ultricies ex. Nullam vitae lectus vitae arcu efficitur auctor id a sem. Mauris congue enim risus, eu gravida mi dignissim ut. Vestibulum tempus urna vel sem eleifend, quis aliquet ligula maximus. Vivamus sagittis dolor eu iaculis interdum. Morbi ex odio, ornare eget erat eu, dapibus accumsan elit. Fusce fermentum orci ante. Etiam dolor urna, dictum a diam nec, ornare tempor nunc. Curabitur imperdiet malesuada augue eget vestibulum.  Securing generative Arts through NFTs. Lorem ipsum amet dolor sit amet, minim altera mucius an eum. Legere antiopam definitiones nam an.",category:{type:"CONFERENCE",categoryTitle:"The digital asset certification",issue:"",volume:"",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio","Yoshua Aenjio","Yoshua Cenjio","Yoshua Denjio","Yoshua Eenjio","Yoshua Fenjio","Yoshua Genjio","Yoshua Henjio","Yoshua Ienjio"],yearPublished:"2017"},{_id:"fake_publication_7",title:"Wireframe-based UI design search through image autoencoder",description:"Securing generative Arts through NFTs",category:{type:"CONFERENCE",categoryTitle:"The digital asset certification",issue:"",volume:"",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio"],yearPublished:"2017"}],n={twitterHandle:"elonmusk",orgName:"Monash",teamName:"MonTeam"},o=[{fullName:"John",position:"Chief Scientist",summary:"John is a chief scientist at MonTeam, working with the top government agencies to fight the pressing issues arising from climate change"},{fullName:"Yoshua Benjio",position:"Chief Data Scientist",summary:"Hailed as one of the founders of Deep Learning, Yoshua works at MonTeam to oversee strategic deep learning project designs"},{fullName:"Jeremy Buffet",position:"Chief Economist",summary:"Jeremy Buffet leads our macro-economic unit in predicting macro factors and their impact on society"}],r={aboutUs:["Nothing \u2014 not the careful logic of mathematics, not statistical models and theories, not the awesome arithmetic power of modern computers \u2014 nothing can substitute here for the flexibility of the informed human mind... Accordingly, both [analysis] approaches and techniques need to be structured so as to facilitate human involvement and intervention.     \u2013 John W. Tukey & Martin B. Wilk, Data Analysis & Statistics, 1966","The mission of the Interactive Data Lab is to enhance people ability to understand and communicate data through the design of new interactive systems for data visualization and analysis. We study the perceptual, cognitive and social factors affecting data analysis in order to improve the efficiency and scale at which expert analysts work, and to lower barriers for non-experts.","Motivating questions include: How might we enable users to transform and integrate data with minimal programming? How can we support expressive and effective visualization designs? Can we build systems to query and visualize massive data sets at interactive rates? How might we enable domain experts to guide machine learning methods to produce better models?"]},s={pages:["PUBLICATIONS","TEAM","ACHIEVEMENTS"],publicationOptions:{layout:"All Publication",sortBy:"Author"},template:{layout:"2",theme:"dark"}},d=[{title:"Organisation of The Year",description:"We have been awarded as the Organisation of The Year for 2019",yearAwarded:2019},{title:"Best Team Vibes",description:"We have been awarded for the Best Team Vibes for 2021",yearAwarded:2021},{title:"Most Hardworking Team",description:"We have been awarded as the Most Hardedworking team for 2020",yearAwarded:2020},{title:"Most Dedicated Team",description:"We have been awarded as the Most Dedicated Team for 2020",yearAwarded:2020},{title:"Participation Award",description:"Our efforts have been recognised as we have been awarded the Participation Award for the year 2018. Adding more information to make thi description longer. Adding more information to make thi description longer. Adding more information to make thi description longer. Adding more information to make thi description longer. Adding more information to make thi description longer. Adding more information to make thi description longer.",yearAwarded:2018},{title:"Most Employable Team",description:"We have been awarded as the Most Employable Team for 2019",yearAwarded:2019}]):(i=l.REACT_APP_TEAM_PUBLICATIONS?JSON.parse(l.REACT_APP_TEAM_PUBLICATIONS):[],n=l.REACT_APP_TEAM_INFO?JSON.parse(l.REACT_APP_TEAM_INFO):null,o=l.REACT_APP_TEAM_MEMBERS?JSON.parse(l.REACT_APP_TEAM_MEMBERS):[],r=l.REACT_APP_TEAM_HOMEPAGE?JSON.parse(l.REACT_APP_TEAM_HOMEPAGE):null,s=l.REACT_APP_TEAM_SITE_METADATA?JSON.parse(l.REACT_APP_TEAM_SITE_METADATA):{pages:[],template:{layout:3,theme:"light"},publicationOptions:{layout:"By Category",sortBy:"Category Title"}},d=l.REACT_APP_TEAM_ACHIEVEMENTS?JSON.parse(l.REACT_APP_TEAM_ACHIEVEMENTS):[])},19:function(e,a,t){"use strict";t.r(a);var i=t(0),n=t.n(i),o=t(15),r=t.n(o),s=t(14),d=(t(24),t(25),t(26),t(27),t(13)),l=t(1),c=d.f.template.layout;r.a.render(Object(l.jsx)(s.a,{children:function(){switch(c){case"1":var e=n.a.lazy((function(){return Promise.all([t.e(1),t.e(3)]).then(t.bind(null,18))}));return Object(l.jsx)(l.Fragment,{children:Object(l.jsx)(n.a.Suspense,{fallback:Object(l.jsx)(l.Fragment,{}),children:Object(l.jsx)(e,{})})});case"2":var a=n.a.lazy((function(){return Promise.all([t.e(1),t.e(7),t.e(9)]).then(t.bind(null,36))}));return Object(l.jsx)(l.Fragment,{children:Object(l.jsx)(n.a.Suspense,{fallback:Object(l.jsx)(l.Fragment,{}),children:Object(l.jsx)(a,{})})});case"3":var i=n.a.lazy((function(){return Promise.all([t.e(1),t.e(8),t.e(10)]).then(t.bind(null,37))}));return Object(l.jsx)(l.Fragment,{children:Object(l.jsx)(n.a.Suspense,{fallback:Object(l.jsx)(l.Fragment,{}),children:Object(l.jsx)(i,{})})});default:var o=n.a.lazy((function(){return Promise.all([t.e(1),t.e(3)]).then(t.bind(null,18))}));return Object(l.jsx)(l.Fragment,{children:Object(l.jsx)(n.a.Suspense,{fallback:Object(l.jsx)(l.Fragment,{}),children:Object(l.jsx)(o,{})})})}}()}),document.getElementById("root"))}},[[19,5,6]]]);
//# sourceMappingURL=main.77d3cac8.chunk.js.map